{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning for robotics\n",
    "This is the initial notebook that you will need to fill out through the semester. \n",
    "### Setup\n",
    "First let's make sure that everything is working properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "assert gym.__version__=='1.0.0',\"You need a newer version of gym\"\n",
    "print(\"Everything seems good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline\n",
    "As teaching a robot how to walk is tricky, you will first test your algorithm on a much simpler task: Balancing an inverted pendulum.\n",
    "This week, you will:\n",
    "- Setup a first enviromnent\n",
    "- Run a random policy\n",
    "- Modify the distribution of this policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the environment\n",
    "envname=\"InvertedPendulum-v5\"\n",
    "env = gym.make_vec(envname,1,render_mode='rgb_array',vectorization_mode='sync')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This environment is called InvertedPendulum, and is running in the Mujoco simulator. You can check what it can do by reading the [documentation](https://gymnasium.farama.org/environments/mujoco/inverted_pendulum/)\n",
    "\n",
    "Your first task is to find what are the state space and the action space. Additionally, answer the following questions:\n",
    "- What is the dimension of the state space?\n",
    "- What is the dimension of the action space?\n",
    "- How could you get these dimentions directly in your code?\n",
    "- When your ran these commands, you should have gotten an array of dimension 2. What does each of the dimension represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Answers_: - What is the dimension of the state space? 4\n",
    "- What is the dimension of the action space? 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()\n",
    "print(\"State space dimension:\", env.single_observation_space.shape)\n",
    "print(\"Action space dimension:\", env.single_action_space.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "Gymnasium is providing a visualisation function, let's try it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_notebook(env,id,title=\"\"):\n",
    "    clear_output(wait=True)\n",
    "    plt.imshow(env.render()[id])\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()\n",
    "render_notebook(env, 0, \"Example state:\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random policy\n",
    "Now you will try to implement a random policy: Uniformely chose a random action at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminated = [False]\n",
    "env.reset()\n",
    "ret=0\n",
    "while not all(terminated):\n",
    "    action = np.random.uniform(-3, 3, size=(1, 1))\n",
    "    _,reward, terminated,truncated,info = env.step(action)\n",
    "    terminated = terminated|truncated\n",
    "    ret+=reward\n",
    "    render_notebook(env,0,f\"{ret=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other distribution\n",
    "This policy is quite terrible, so let's try to improve it by using a gaussian distribution instead. Test several standard deviations and see which one works best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "terminated = [False]\n",
    "std = 0.1\n",
    "env.reset()\n",
    "ret=0\n",
    "while not all(terminated):\n",
    "    action = np.random.normal(0, std, size=(1, 1))\n",
    "    _,reward, terminated,truncated,info = env.step(action)\n",
    "    terminated = terminated|truncated\n",
    "    ret+=reward\n",
    "    render_notebook(env,0,f\"{ret=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes this first part of the project, next week we will try to implement a feedback controler in this system. \n",
    "In the meantime, feel free to get more confortable with the documentation of gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FeedBack Policy u=-Kx avec K a determiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = np.array([[-1.0 , -2.0, -3.0, -4.0]]) \n",
    "terminated = [False]\n",
    "env.reset()\n",
    "ret=0\n",
    "while not all(terminated):\n",
    "    action = action = -np.dot(K, obs.T)\n",
    "    obs,reward, terminated,truncated,info = env.step(action)\n",
    "    terminated = terminated|truncated\n",
    "    ret+=reward\n",
    "    render_notebook(env,0,f\"{ret=}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(K):\n",
    "    obs, info = env.reset()\n",
    "    terminated = [False]\n",
    "    ret = 0\n",
    "    while not all(terminated):\n",
    "        action = -np.dot(K, obs.T)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        terminated = terminated | truncated\n",
    "        ret += reward\n",
    "    return ret\n",
    "\n",
    "bret = -np.inf  \n",
    "bK = None\n",
    "num = 10000\n",
    "for i in range(num):\n",
    "    K_cand = np.random.uniform(low=-5, high=5, size=(1, 4))\n",
    "    cret = simulate(K_cand)\n",
    "    if cret > bret:\n",
    "        bret = cret\n",
    "        bK = K_cand \n",
    "    if np.isclose(cret.item(), 1000.0):\n",
    "        print(f\"Iteration {i}: reward = {cret}, best reward = {bret}\")\n",
    "        break\n",
    "    print(f\"Iteration {i}: reward = {cret}, best reward = {bret}\")\n",
    "\n",
    "print(f\"Best K found : {bK}\", f\"best Reward = {bret}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()\n",
    "terminated = [False]\n",
    "ret = 0\n",
    "\n",
    "while not all(terminated):\n",
    "    action = -np.dot(bK, obs.T)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    terminated = terminated | truncated\n",
    "    ret += reward\n",
    "    render_notebook(env, 0, f\"{ret=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=1\n",
    "test=test+1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
