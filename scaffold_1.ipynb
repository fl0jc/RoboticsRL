{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning for robotics\n",
    "This is the initial notebook that you will need to fill out through the semester. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "First let's make sure that everything is working properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "from torch.distributions import Normal\n",
    "from IPython.display import clear_output\n",
    "assert gym.__version__=='1.0.0',\"You need a newer version of gym\"\n",
    "print(\"Everything seems good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline\n",
    "As teaching a robot how to walk is tricky, you will first test your algorithm on a much simpler task: Balancing an inverted pendulum.\n",
    "This week, you will:\n",
    "- Setup a first enviromnent\n",
    "- Run a random policy\n",
    "- Modify the distribution of this policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the environment\n",
    "envname=\"InvertedPendulum-v5\" \n",
    "env = gym.make_vec(envname,num_envs=1,render_mode='rgb_array',vectorization_mode='sync')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This environment is called InvertedPendulum, and is running in the Mujoco simulator. You can check what it can do by reading the [documentation](https://gymnasium.farama.org/environments/mujoco/inverted_pendulum/)\n",
    "\n",
    "Your first task is to find what are the state space and the action space. Additionally, answer the following questions:\n",
    "- What is the dimension of the state space?\n",
    "- What is the dimension of the action space?\n",
    "- How could you get these dimentions directly in your code?\n",
    "- When your ran these commands, you should have gotten an array of dimension 2. What does each of the dimension represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Answers_: - What is the dimension of the state space? 4\n",
    "- What is the dimension of the action space? 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()\n",
    "print(\"State space dimension:\", env.single_observation_space.shape)\n",
    "print(\"Action space dimension:\", env.single_action_space.shape)\n",
    "print(\"Action space low:\", env.single_action_space.low)\n",
    "print(\"Action space high:\", env.single_action_space.high)\n",
    "\n",
    "state_dim = env.single_observation_space.shape[0]\n",
    "action_dim = env.single_action_space.shape[0]\n",
    "num_params = state_dim * action_dim\n",
    "\n",
    "print(\"Number of parameters (linear model without bias):\", num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "Gymnasium is providing a visualisation function, let's try it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_notebook(env,id,title=\"\"):\n",
    "    clear_output(wait=True)\n",
    "    plt.imshow(env.render()[id])\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()\n",
    "render_notebook(env, 0, \"Example state:\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random policy\n",
    "Now you will try to implement a random policy: Uniformely chose a random action at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminated = [False]\n",
    "env.reset()\n",
    "ret=0\n",
    "while not all(terminated):\n",
    "    action = np.random.uniform(-3, 3, size= env.action_space.shape)\n",
    "    _,reward, terminated,truncated,info = env.step(action)\n",
    "    terminated = terminated|truncated\n",
    "    ret+=reward\n",
    "    render_notebook(env,0,f\"{ret=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other distribution\n",
    "This policy is quite terrible, so let's try to improve it by using a gaussian distribution instead. Test several standard deviations and see which one works best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "terminated = [False]\n",
    "std = 0.1\n",
    "env.reset()\n",
    "ret=0\n",
    "while not all(terminated):\n",
    "    action = np.random.normal(0, std, size=env.action_space.shape)\n",
    "    _,reward, terminated,truncated,info = env.step(action)\n",
    "    terminated = terminated|truncated\n",
    "    ret+=reward\n",
    "    render_notebook(env,0,f\"{ret=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes this first part of the project, next week we will try to implement a feedback controler in this system. \n",
    "In the meantime, feel free to get more confortable with the documentation of gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FeedBack Policy u=-Kx avec K a determiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = np.array([[1.0 , 2.0, 3.0, 4.0]]) \n",
    "terminated = [False]\n",
    "env.reset()\n",
    "ret=0\n",
    "while not all(terminated):\n",
    "    action = np.dot(K, obs.T)\n",
    "    obs,reward, terminated,truncated,info = env.step(action)\n",
    "    terminated = terminated|truncated\n",
    "    ret+=reward\n",
    "    render_notebook(env,0,f\"{ret=}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(K):\n",
    "    obs, info = env.reset()\n",
    "    terminated = [False]\n",
    "    ret = 0\n",
    "    while not all(terminated):\n",
    "        action = np.dot(K, obs[0])\n",
    "        obs, reward, terminated, truncated, info = env.step([action])\n",
    "        terminated = terminated | truncated\n",
    "        ret += reward\n",
    "    return ret\n",
    "\n",
    "bret = -np.inf  \n",
    "bK = None\n",
    "num = 100000\n",
    "for i in range(num):\n",
    "    K_cand = np.random.uniform(low=-5, high=5, size=(\n",
    "                                                     env.action_space.shape[1],\n",
    "                                                     env.single_observation_space.shape[0]))\n",
    "    cret = simulate(K_cand)\n",
    "    if cret > bret:\n",
    "        bret = cret\n",
    "        bK = K_cand \n",
    "    if np.isclose(cret.item(), 1000.0):\n",
    "        print(f\"Iteration {i}: reward = {cret}, best reward = {bret}\")\n",
    "        break\n",
    "    print(f\"Iteration {i}: reward = {cret}, best reward = {bret}\")\n",
    "\n",
    "print(f\"Best K found : {bK}\", f\"best Reward = {bret}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()\n",
    "terminated = [False]\n",
    "ret = 0\n",
    "\n",
    "while not all(terminated):\n",
    "    action = np.dot(bK, obs.T)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    print(obs)\n",
    "    terminated = terminated | truncated\n",
    "    ret += reward\n",
    "    render_notebook(env, 0, f\"{ret=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement learning (RL) Week 3 (test) Flo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#policy : a= K s   => deterministic, linear, parameterized policy \n",
    "#grad compute with finite difference Not WORKING\n",
    "\n",
    "def episode(K):\n",
    "    obs, info = env.reset()\n",
    "    terminated = [False]\n",
    "    ret = 0.0\n",
    "    while not all(terminated):\n",
    "        action = -np.dot(K, obs[0])\n",
    "        action = np.clip(action, -3.0, 3.0)\n",
    "        obs, reward, terminated, truncated, info = env.step([action])\n",
    "        terminated = terminated | truncated\n",
    "        ret += float(reward[0]) \n",
    "        \n",
    "    return ret\n",
    "\n",
    "\n",
    "def compute_J(K, n_episodes=100):\n",
    "    total = 0.0\n",
    "    for i in range(n_episodes):\n",
    "        total += episode(K)\n",
    "    return total / n_episodes\n",
    "\n",
    "\n",
    "def compute_gradJ(K, h=0.01):\n",
    "    gradJ = np.zeros_like(K)\n",
    "    for i in range(K.shape[0]):\n",
    "        for j in range(K.shape[1]):\n",
    "            K_plus = K.copy()\n",
    "            K_minus = K.copy()\n",
    "\n",
    "            K_plus[i, j]  += h\n",
    "            K_minus[i, j] -= h\n",
    "\n",
    "            r_plus  = compute_J(K_plus)\n",
    "            r_minus = compute_J(K_minus)\n",
    "\n",
    "            gradJ[i, j] = (r_plus - r_minus) / (2.0 * h)\n",
    "\n",
    "    return gradJ\n",
    "\n",
    "\n",
    "def training(a, iterations, h):\n",
    "    K = np.random.uniform(low=-5, high=5, size=(env.action_space.shape[1],env.single_observation_space.shape[0]))\n",
    "    #K = np.array([[-0.0, -4, -1, -2.3]]).reshape(env.action_space.shape[1],env.single_observation_space.shape[0])\n",
    "    best_reward = -np.inf\n",
    "    best_K = K.copy()\n",
    "    iteration_reached = None\n",
    "    target = 1000.0\n",
    "    for i in range(iterations):\n",
    "        gradJ = compute_gradJ(K, h=h)\n",
    "        K = K + a * gradJ\n",
    "        cret = compute_J(K)\n",
    "        if cret > best_reward:\n",
    "            best_reward = cret\n",
    "            best_K = K.copy()\n",
    "        print(f\"Iteration {i}: reward = {cret:.2f}, best reward = {best_reward:.2f}\")\n",
    "        print(K)\n",
    "        print(gradJ)\n",
    "        if cret >= target:\n",
    "            iteration_reached = i\n",
    "            print(f\">>> Cible de reward {target} atteinte à l'itération {i}.\")\n",
    "            break\n",
    "    print(f\"Best K found: {best_K}\")\n",
    "    print(f\"Best reward: {best_reward:.2f}\")\n",
    "\n",
    "    return best_K, best_reward, iteration_reached\n",
    "\n",
    "\n",
    "\n",
    "training(a=0.001, iterations=100000, h=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##RL linéaire\n",
    "K = np.random.uniform(low=-3, high=3, size=(env.action_space.shape[1],env.single_observation_space.shape[0]))\n",
    "gamma = 0.99\n",
    "alpha = 0.01 \n",
    "sigma = 0.5\n",
    "bret = -np.inf  \n",
    "bK = None\n",
    "\n",
    "def policy(s):\n",
    "    return np.random.normal(np.dot(K,s),sigma)\n",
    "\n",
    "def compute_log_gradient(s, a):\n",
    "    mu = np.dot(K, s)\n",
    "    return ((a - mu) * s / sigma**2) \n",
    "\n",
    "for episode in range(5000):\n",
    "    obs, info = env.reset()\n",
    "    terminated = [False]\n",
    "    ret = 0.0\n",
    "    obss,actions,rewards=[],[],[]\n",
    "    while not all(terminated):\n",
    "        s = obs[0]\n",
    "        action = policy(s)\n",
    "        action = np.clip(action, -3.0, 3.0)\n",
    "        next_obs, reward, terminated, truncated, info = env.step([action])\n",
    "        terminated = terminated | truncated\n",
    "        obss.append(s)\n",
    "        actions.append(action)  \n",
    "        rewards.append(reward)\n",
    "        ret += reward\n",
    "        obs = next_obs\n",
    "\n",
    "    if ret >= bret:\n",
    "        bret = ret \n",
    "        bK = K.copy()\n",
    "\n",
    "    returns = np.zeros(len(rewards))\n",
    "    G = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        G = rewards[t] + gamma * G\n",
    "        returns[t] = G\n",
    "    \n",
    "    alpha = 0.0005 / (1 + 0.00005 * episode)\n",
    "    \n",
    "    for t in range(len(obss)):\n",
    "        grad = compute_log_gradient(obss[t], actions[t])  \n",
    "        K += alpha * grad * returns[t]  \n",
    "    print(f\"Episode {episode}, Total Reward: {ret}, Best Reward: {bret}, K:{K}\" )\n",
    "    \n",
    "print(bK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()\n",
    "terminated = [False]\n",
    "ret = 0\n",
    "\n",
    "while not all(terminated):\n",
    "    action = np.dot(bK, obs.T)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    terminated = terminated | truncated\n",
    "    ret += reward\n",
    "    render_notebook(env, 0, f\"{ret=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bK.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##NON linear Policy using tanh version qui fonctionne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = np.random.uniform(low=-1, high=1, size=(env.action_space.shape[1],env.single_observation_space.shape[0]))\n",
    "gamma = 0.99\n",
    "sigma = 0.5\n",
    "bret = -np.inf  \n",
    "bK = None\n",
    "alpha = 0.000005\n",
    "patience = 200\n",
    "cc=0\n",
    "maxret=1000\n",
    "\n",
    "\n",
    "def policy(s):\n",
    "    return np.random.normal(env.single_action_space.high * np.tanh(np.dot(K, s)), sigma)\n",
    "\n",
    "def compute_log_gradient(s, a):\n",
    "    z  = np.dot(K, s)         \n",
    "    mu = env.single_action_space.high * np.tanh(z)       #env.single_action_space.high = 3 for InvertedPendulum-v5\n",
    "    return np.outer(((a - mu) * env.single_action_space.high * (1 - np.tanh(z)**2)) / sigma**2,s)\n",
    "for episode in range(100000):\n",
    "    obs, info = env.reset()\n",
    "    terminated = [False]\n",
    "    ret = 0.0\n",
    "    obss,actions,rewards=[],[],[]\n",
    "    while not all(terminated):\n",
    "        s = obs[0]\n",
    "        action = policy(s)\n",
    "        action = np.clip(action, env.single_action_space.low, env.single_action_space.high)\n",
    "        next_obs, reward, terminated, truncated, info = env.step([action])\n",
    "        terminated = terminated | truncated\n",
    "        obss.append(s)\n",
    "        actions.append(action)  \n",
    "        rewards.append(reward)\n",
    "        ret += reward\n",
    "        obs = next_obs\n",
    "\n",
    "    if ret >= bret:\n",
    "        bret = ret \n",
    "        bK = K.copy()\n",
    "\n",
    "    returns = np.zeros(len(rewards))\n",
    "    G = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        G = rewards[t] + gamma * G\n",
    "        returns[t] = G\n",
    "    \n",
    "    for t in range(len(obss)):\n",
    "        grad = compute_log_gradient(obss[t], actions[t])  \n",
    "        K += alpha * grad * returns[t]\n",
    "    print(f\"Episode {episode}, Total Reward: {ret}, Best Reward: {bret}, K:{K}\" )\n",
    "    if ret>=maxret:\n",
    "        cc+=1\n",
    "    else:\n",
    "        cc=0\n",
    "    \n",
    "    if cc>=patience:\n",
    "        print(f\"Early stopping: {patience} consecutive episodes with reward >= {maxret}\")\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()\n",
    "terminated = [False]\n",
    "ret = 0\n",
    "\n",
    "while not all(terminated):\n",
    "    action = np.dot(K, obs.T)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    terminated = terminated | truncated\n",
    "    ret += reward\n",
    "    render_notebook(env, 0, f\"{ret=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement learning (RL) Week 3 (test) Selim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " One run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = env.observation_space.shape[1]\n",
    "action_dim = env.action_space.shape[1]\n",
    "\n",
    "# Initialize policy parameters\n",
    "theta = np.random.randn(action_dim,state_dim)*0.1\n",
    "best_reward = -np.inf\n",
    "best_theta = None\n",
    "action_std=0.25\n",
    "num_episodes=5000\n",
    "gamma=0.99\n",
    "time = 0\n",
    "\n",
    "def policy(state):\n",
    "    mean = np.dot(theta,state)\n",
    "    action=np.random.normal(mean, action_std)\n",
    "    return action\n",
    "\n",
    "def compute_log_gradient(state, action):\n",
    "    mean = np.dot(theta, state)\n",
    "    log_gradient = np.outer((action - mean), state) / action_std**2\n",
    "    return log_gradient\n",
    "\n",
    "gradient_norms = []\n",
    "reward_each_episode=[]\n",
    "for episode in range(num_episodes):\n",
    "    obs, info = env.reset()\n",
    "    state = obs[0]\n",
    "    trajectory = [] \n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = policy(state)\n",
    "        print(action)\n",
    "        next_state, reward, done, truncated, _ = env.step([action])\n",
    "        done = done or truncated\n",
    "        trajectory.append((state, action, reward))  # Store in trajectory\n",
    "        total_reward += reward\n",
    "        state = next_state[0]\n",
    "\n",
    "    # Track best performance\n",
    "    if total_reward == best_reward:\n",
    "        time += 1  \n",
    "    else:\n",
    "        time = 0  \n",
    "\n",
    "    if time >= 20:  # Stop if no improvement for 20 episodes\n",
    "        break\n",
    "    \n",
    "    if total_reward > best_reward:  # Update best_reward only if strictly better\n",
    "        best_reward = total_reward\n",
    "        best_theta = theta  \n",
    "\n",
    "    returns = np.zeros(len(trajectory))\n",
    "    G = 0\n",
    "    for t in reversed(range(len(trajectory))):\n",
    "        G = trajectory[t][2] + gamma * G \n",
    "        returns[t] = G\n",
    "    \n",
    "    alpha = 0.001\n",
    "    for t, (state, action, _) in enumerate(trajectory):\n",
    "        grad = compute_log_gradient(state, action)\n",
    "        theta += alpha * grad * returns[t]\n",
    "        \n",
    "        grad_norm = 0\n",
    "        grad_norm = np.linalg.norm(grad) \n",
    "        gradient_norms.append(grad_norm)  \n",
    "    print(theta)     \n",
    "    reward_each_episode.append(total_reward)\n",
    "    print(f\"Episode {episode}, Total Reward: {total_reward}, Best Reward: {best_reward}\")\n",
    "\n",
    "theta = best_theta\n",
    "np.save('trained_theta.npy', theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained parameters\n",
    "theta = np.load('trained_theta.npy')\n",
    "obs, info = env.reset()\n",
    "terminated = [False]\n",
    "ret = 0\n",
    "while not all(terminated):\n",
    "    state=obs[0]\n",
    "    action = np.clip(np.dot(theta,state), env.action_space.low, env.action_space.high)  # Get action from the trained policy\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    terminated = terminated | truncated\n",
    "    ret += reward\n",
    "    render_notebook(env, 0, f\"{ret=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = env.observation_space.shape[1]\n",
    "action_dim = env.action_space.shape[1]\n",
    "\n",
    "action_std=0.25\n",
    "num_episodes=5000\n",
    "gamma=0.99\n",
    "alphas=[0.00001,0.0001,0.001]\n",
    "number_of_runs=10\n",
    "\n",
    "def policy(state):\n",
    "    mean = np.dot(theta,state)\n",
    "    action=np.random.normal(mean, action_std)\n",
    "    return action\n",
    "\n",
    "def compute_log_gradient(state, action): \n",
    "    mean = np.dot(theta, state)\n",
    "    log_gradient = np.outer((action - mean), state) / action_std**2\n",
    "    return log_gradient\n",
    "\n",
    "results_file = \"saved_results_alpha.pkl\"\n",
    "overwrite_existing = True  # set to True if you want to force rerun\n",
    "\n",
    "# Automatically load if it exists and we don't want to overwrite\n",
    "if os.path.exists(results_file) and not overwrite_existing:\n",
    "    print(\"Loading previously saved results...\")\n",
    "    with open(results_file, \"rb\") as f:\n",
    "        results = pickle.load(f)\n",
    "else:\n",
    "    print(\"Starting new experiment. Previous results will be replaced.\")\n",
    "    results = {alpha: [] for alpha in alphas}\n",
    "\n",
    "    for alpha in alphas:\n",
    "        print(f\"\\n=== Testing alpha = {alpha} ===\")\n",
    "        run_rewards = []\n",
    "        for run in range(number_of_runs):\n",
    "            print(f\"\\n  -> Run {run + 1}/{number_of_runs} (alpha={alpha})\")\n",
    "            time = 0\n",
    "            theta = np.random.randn(action_dim, state_dim) * 0.1\n",
    "            best_reward = -np.inf\n",
    "            best_theta = None\n",
    "            reward_each_episode = []\n",
    "\n",
    "            for episode in range(num_episodes):\n",
    "                obs, info = env.reset()\n",
    "                state = obs[0]\n",
    "                trajectory = []\n",
    "                total_reward = 0\n",
    "                done = False\n",
    "                while not done:\n",
    "                    action = policy(state)\n",
    "                    next_state, reward, done, truncated, _ = env.step([action])\n",
    "                    done = done or truncated\n",
    "                    trajectory.append((state, action, reward))\n",
    "                    total_reward += reward\n",
    "                    state = next_state[0]\n",
    "\n",
    "                if total_reward == best_reward:\n",
    "                    time += 1\n",
    "                else:\n",
    "                    time = 0\n",
    "\n",
    "                if time >= 100:\n",
    "                    break\n",
    "\n",
    "                if total_reward > best_reward:\n",
    "                    best_reward = total_reward\n",
    "                    best_theta = theta\n",
    "\n",
    "                returns = np.zeros(len(trajectory))\n",
    "                G = 0\n",
    "                for t in reversed(range(len(trajectory))):\n",
    "                    G = trajectory[t][2] + gamma * G\n",
    "                    returns[t] = G\n",
    "\n",
    "                for t, (state, action, _) in enumerate(trajectory):\n",
    "                    grad = compute_log_gradient(state, action)\n",
    "                    theta += alpha * grad * returns[t]\n",
    "\n",
    "                reward_each_episode.append(total_reward)\n",
    "\n",
    "            run_rewards.append(reward_each_episode)\n",
    "            print(f\"  -> Run {run + 1} finished. Best Reward: {best_reward}\")\n",
    "\n",
    "        results[alpha] = run_rewards\n",
    "        print(f\"=== Finished testing alpha = {alpha} ===\\n\")\n",
    "\n",
    "    # Save results\n",
    "    with open(results_file, \"wb\") as f:\n",
    "        pickle.dump(results, f)\n",
    "    print(f\"Results saved to {results_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved results from file\n",
    "with open(\"saved_results_alpha.pkl\", \"rb\") as f:\n",
    "        results = pickle.load(f)\n",
    "\n",
    "# Now plot from loaded data\n",
    "for alpha in results:\n",
    "    if not results.get(alpha) or len(results[alpha]) == 0:\n",
    "        print(f\"Skipping action_std={alpha} (no runs available)\")\n",
    "        continue\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "\n",
    "    # Find the longest and shortest run lengths\n",
    "    max_len = max(len(run) for run in results[alpha])\n",
    "    min_len = min(len(run) for run in results[alpha])\n",
    "    fastest_run_idx = [i for i, run in enumerate(results[alpha]) if len(run) == min_len][0]\n",
    "\n",
    "    padded_runs = []\n",
    "    for run_rewards in results[alpha]:\n",
    "        run_rewards = np.array(run_rewards).flatten()\n",
    "        pad_length = max_len - len(run_rewards)\n",
    "        if pad_length > 0:\n",
    "            padded = np.concatenate([run_rewards, np.full(pad_length, 1000.0)])\n",
    "        else:\n",
    "            padded = run_rewards\n",
    "        padded_runs.append(padded)\n",
    "\n",
    "    all_runs = np.stack(padded_runs)\n",
    "\n",
    "    # Compute mean and std\n",
    "    mean_rewards = np.mean(all_runs,axis=0)\n",
    "    lower_percentile = np.percentile(all_runs, 5, axis=0)\n",
    "    upper_percentile = np.percentile(all_runs,95, axis=0)\n",
    "\n",
    "    episodes = np.arange(len(mean_rewards))\n",
    "    plt.plot(episodes, mean_rewards, label=f\"Mean Reward (alpha={alpha})\", color=\"green\")\n",
    "    plt.fill_between(episodes, lower_percentile, upper_percentile, alpha=0.2, color=\"green\", label=\"5–95 percentile\")\n",
    "    # Mark the fastest run\n",
    "    plt.axvline(x=min_len, color=\"red\", linestyle=\"--\", linewidth=1.5, label=f\"Fastest run (ep. {min_len})\")\n",
    "\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(f\"Mean Learning Curve for alpha={alpha}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"mean_curve_alpha_{alpha}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action std plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = env.observation_space.shape[1]\n",
    "action_dim = env.action_space.shape[1]\n",
    "\n",
    "action_stds=[0.2,0.3,0.4]\n",
    "num_episodes=5000\n",
    "gamma=0.99\n",
    "alpha=0.0001\n",
    "number_of_runs=10\n",
    "\n",
    "def policy(state):\n",
    "    mean = np.dot(theta,state)\n",
    "    action=np.random.normal(mean, action_std)\n",
    "    return action\n",
    "\n",
    "def compute_log_gradient(state, action): \n",
    "    mean = np.dot(theta, state)\n",
    "    log_gradient = np.outer((action - mean), state) / action_std**2\n",
    "    return log_gradient\n",
    "\n",
    "results_file = \"saved_results_actionstd.pkl\"\n",
    "overwrite_existing = True  # set to True if you want to force rerun\n",
    "\n",
    "# Automatically load if it exists and we don't want to overwrite\n",
    "if os.path.exists(results_file) and not overwrite_existing:\n",
    "    print(\"Loading previously saved results...\")\n",
    "    with open(results_file, \"rb\") as f:\n",
    "        results = pickle.load(f)\n",
    "else:\n",
    "    print(\"Starting new experiment. Previous results will be replaced.\")\n",
    "    results = {action_std: [] for action_std in action_stds}\n",
    "\n",
    "    for action_std in action_stds:\n",
    "        print(f\"\\n=== Testing action_std = {action_std} ===\")\n",
    "        run_rewards = []\n",
    "        for run in range(number_of_runs):\n",
    "            print(f\"\\n  -> Run {run + 1}/{number_of_runs} (action_std={action_std})\")\n",
    "            time = 0\n",
    "            theta = np.random.randn(action_dim, state_dim) * 0.1\n",
    "            best_reward = -np.inf\n",
    "            best_theta = None\n",
    "            reward_each_episode = []\n",
    "\n",
    "            for episode in range(num_episodes):\n",
    "                obs, info = env.reset()\n",
    "                state = obs[0]\n",
    "                trajectory = []\n",
    "                total_reward = 0\n",
    "                done = False\n",
    "                while not done:\n",
    "                    action = policy(state)\n",
    "                    next_state, reward, done, truncated, _ = env.step([action])\n",
    "                    done = done or truncated\n",
    "                    trajectory.append((state, action, reward))\n",
    "                    total_reward += reward\n",
    "                    state = next_state[0]\n",
    "\n",
    "                if total_reward == best_reward:\n",
    "                    time += 1\n",
    "                else:\n",
    "                    time = 0\n",
    "\n",
    "                if time >= 100:\n",
    "                    break\n",
    "\n",
    "                if total_reward > best_reward:\n",
    "                    best_reward = total_reward\n",
    "                    best_theta = theta\n",
    "\n",
    "                returns = np.zeros(len(trajectory))\n",
    "                G = 0\n",
    "                for t in reversed(range(len(trajectory))):\n",
    "                    G = trajectory[t][2] + gamma * G\n",
    "                    returns[t] = G\n",
    "\n",
    "                for t, (state, action, _) in enumerate(trajectory):\n",
    "                    grad = compute_log_gradient(state, action)\n",
    "                    theta += alpha * grad * returns[t]\n",
    "\n",
    "                reward_each_episode.append(total_reward)\n",
    "\n",
    "            run_rewards.append(reward_each_episode)\n",
    "            print(f\"  -> Run {run + 1} finished. Best Reward: {best_reward}\")\n",
    "\n",
    "        results[action_std] = run_rewards\n",
    "        print(f\"=== Finished testing std = {action_std} ===\\n\")\n",
    "\n",
    "    # Save results\n",
    "    with open(results_file, \"wb\") as f:\n",
    "        pickle.dump(results, f)\n",
    "    print(f\"Results saved to {results_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved results from file\n",
    "with open(\"saved_results_actionstd.pkl\", \"rb\") as f:\n",
    "        results = pickle.load(f)\n",
    "\n",
    "# Now plot from loaded data\n",
    "for action_std in results:\n",
    "    if not results.get(action_std) or len(results[action_std]) == 0:\n",
    "        print(f\"Skipping action_std={action_std} (no runs available)\")\n",
    "        continue\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "\n",
    "    # Find the longest and shortest run lengths\n",
    "    max_len = max(len(run) for run in results[action_std])\n",
    "    min_len = min(len(run) for run in results[action_std])\n",
    "    fastest_run_idx = [i for i, run in enumerate(results[action_std]) if len(run) == min_len][0]\n",
    "\n",
    "    padded_runs = []\n",
    "    for run_rewards in results[action_std]:\n",
    "        run_rewards = np.array(run_rewards).flatten()\n",
    "        pad_length = max_len - len(run_rewards)\n",
    "        if pad_length > 0:\n",
    "            padded = np.concatenate([run_rewards, np.full(pad_length, 1000.0)])\n",
    "        else:\n",
    "            padded = run_rewards\n",
    "        padded_runs.append(padded)\n",
    "\n",
    "    all_runs = np.stack(padded_runs)\n",
    "\n",
    "    mean_rewards = np.mean(all_runs,axis=0)\n",
    "    lower_percentile = np.percentile(all_runs, 5, axis=0)\n",
    "    upper_percentile = np.percentile(all_runs,95, axis=0)\n",
    "\n",
    "    episodes = np.arange(len(mean_rewards))\n",
    "    plt.plot(episodes, mean_rewards, label=f\"Mean Reward (action_std={action_std})\", color=\"green\")\n",
    "    plt.fill_between(episodes, lower_percentile, upper_percentile, alpha=0.2, color=\"green\", label=\"5–95 percentile\")\n",
    "    # Mark the fastest run\n",
    "    plt.axvline(x=min_len, color=\"red\", linestyle=\"--\", linewidth=1.5, label=f\"Fastest run (ep. {min_len})\")\n",
    "\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(f\"Mean Learning Curve for action_std={action_std}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"mean_curve_action_std_{action_std}.png\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LQR Loi optimale mais instable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import solve_discrete_are\n",
    "\n",
    "def solve_discounted_lqr(A, B, Q, R, gamma, max_iter=10000, tol=1e-10):\n",
    "    n = A.shape[0]\n",
    "    P = Q.copy()\n",
    "    for _ in range(max_iter):\n",
    "        S = R + gamma*(B.T @ P @ B)\n",
    "        S_inv = np.linalg.inv(S)\n",
    "        P_next = Q + gamma*(A.T @ P @ A) \\\n",
    "               - gamma**2 * (A.T @ P @ B) @ S_inv @ (B.T @ P @ A)\n",
    "        if np.linalg.norm(P_next - P) < tol:\n",
    "            P = P_next\n",
    "            break\n",
    "        P = P_next\n",
    "\n",
    "    S = R + gamma*(B.T @ P @ B)\n",
    "    S_inv = np.linalg.inv(S)\n",
    "    K = -S_inv @ (gamma * B.T @ P @ A)\n",
    "    return P, K\n",
    "\n",
    "def solve_lqr_classic(A, B, Q, R):\n",
    "\n",
    "    P = solve_discrete_are(A, B, Q, R)\n",
    "    \n",
    "    S_inv = np.linalg.inv(R + B.T @ P @ B)\n",
    "    K = -S_inv @ (B.T @ P @ A)\n",
    "    \n",
    "    return P, K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A and B given\n",
    "\n",
    "A= np.array([[ 1.00000000e+00, -2.35296084e-03,  3.99338571e-02, 1.21166283e-04],\n",
    "             [-1.38777878e-16,  1.02354968e+00,  1.52695351e-04, 3.87872361e-02],\n",
    "             [ 1.19739288e-15, -1.16537817e-01,  9.96701460e-01, 5.20975868e-03],\n",
    "             [-5.46784840e-15,  1.16687611e+00,  7.56271952e-03, 9.47825276e-01]])\n",
    "B = np.array([[ 0.00664611],\n",
    "              [-0.01530383],\n",
    "              [ 0.33142047],\n",
    "              [-0.7575789 ]])\n",
    "\n",
    "Q = np.diag([1.0, 1.0, 1000.0, 1.0])\n",
    "R = np.array([[0.1]])\n",
    "#A = np.array([[2]])\n",
    "#B = np.array([[1]])\n",
    "#Q = np.array([[1]])\n",
    "#R = np.array([[1]])\n",
    "found_controller = False\n",
    "for gamma in np.arange(1, 0, -0.01):\n",
    "    P_sol, K_sol = solve_discounted_lqr(A, B, Q, R, gamma)\n",
    "    A_cl = A + B @ K_sol \n",
    "    eigvals, _ = np.linalg.eig(A_cl)\n",
    "    spectral_radius = np.max(np.abs(eigvals))\n",
    "    print(f\"gamma = {gamma:.4f} | Spectral Radius = {spectral_radius:.4f}\")\n",
    "    if spectral_radius > 1.0:\n",
    "        print(f\"Succes : For gamma = {gamma:.4f}, closed-loop system unstable (spectral radius = {spectral_radius:.4f}).\")\n",
    "        found_controller = True\n",
    "        break\n",
    "\n",
    "if not found_controller:\n",
    "    print(\"No controller found with eigenvalues > 1 in the gamma interval [0,1].\")\n",
    "\n",
    "obs, info = env.reset()\n",
    "terminated = [False]\n",
    "ret = 0\n",
    "while not all(terminated):\n",
    "    action = np.dot(K_sol, obs.T)\n",
    "    action = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    terminated = terminated | truncated\n",
    "    ret += reward\n",
    "print(f\"Reward total dans l'environnement : {ret}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A= np.array([[ 1.00000000e+00, -2.35296084e-03,  3.99338571e-02, 1.21166283e-04],\n",
    "             [-1.38777878e-16,  1.02354968e+00,  1.52695351e-04, 3.87872361e-02],\n",
    "             [ 1.19739288e-15, -1.16537817e-01,  9.96701460e-01, 5.20975868e-03],\n",
    "             [-5.46784840e-15,  1.16687611e+00,  7.56271952e-03, 9.47825276e-01]])\n",
    "B = np.array([[ 0.00664611],\n",
    "              [-0.01530383],\n",
    "              [ 0.33142047],\n",
    "              [-0.7575789 ]])\n",
    "\n",
    "Q = np.diag([1.0, 1.0, 100.0, 1.0])\n",
    "R = np.array([[0.1]])\n",
    "print(f\"gamma = {gamma:.4f}\")\n",
    "P_sol, K_sol = solve_discounted_lqr(A, B, Q, R,gamma)\n",
    "print(\"P matrix:\\n\", P_sol)\n",
    "print(\"K gain:\\n\", K_sol)\n",
    "\n",
    "A_cl = A + B @ K_sol \n",
    "eigvals, _ = np.linalg.eig(A_cl)\n",
    "print(\"closed-loop eigenvalues:\\n\", eigvals)\n",
    "\n",
    "spectral_radius = np.max(np.abs(eigvals))\n",
    "print(f\"Spectral Radius = {spectral_radius}\")\n",
    "if spectral_radius < 1.0:\n",
    "    print(\"=> stable\")\n",
    "else:\n",
    "    print(\"=> unstable.\")\n",
    "\n",
    "obs, info = env.reset()\n",
    "terminated = [False]\n",
    "ret = 0\n",
    "while not all(terminated):\n",
    "    action = np.dot(K_sol, obs.T) \n",
    "    action = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    terminated = terminated | truncated\n",
    "    ret += reward\n",
    "print(f\"Total reward in env: {ret}\")\n",
    "# We get something stable localy stable (ie spectral radius of closed systems <1) but when we launch the env it's unstable after approx 25 iterations.\n",
    "# The discount create a myopic controller, it does not stabilize the system globally (for 1000 iterations) but it stabilizes it for the launch (for the first iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##RL linéaire\n",
    "K = np.array([-3.00969248e-06,7.93095359e+00,9.70670245e-03,1.36614601e+00]).reshape(env.action_space.shape[1],env.single_observation_space.shape[0])\n",
    "print(\"K =\",K)\n",
    "print(f\"gamma = {gamma:.4f}\")\n",
    "sigma = 0.25\n",
    "bret = -np.inf  \n",
    "bK = None\n",
    "\n",
    "def policy(s):\n",
    "    return np.random.normal(np.dot(K,s),sigma)\n",
    "\n",
    "def compute_log_gradient(s, a):\n",
    "    mu = np.dot(K, s)\n",
    "    return ((a - mu) * s / sigma**2) \n",
    "\n",
    "for episode in range(5000):\n",
    "    obs, info = env.reset()\n",
    "    terminated = [False]\n",
    "    ret = 0.0\n",
    "    obss,actions,rewards=[],[],[]\n",
    "    while not all(terminated):\n",
    "        s = obs[0]\n",
    "        action = policy(s)\n",
    "        action = np.clip(action, -3.0, 3.0)\n",
    "        next_obs, reward, terminated, truncated, info = env.step([action])\n",
    "        terminated = terminated | truncated\n",
    "        obss.append(s)\n",
    "        actions.append(action)  \n",
    "        rewards.append(reward)\n",
    "        ret += reward\n",
    "        obs = next_obs\n",
    "\n",
    "    if ret >= bret:\n",
    "        bret = ret \n",
    "        bK = K.copy()\n",
    "\n",
    "    returns = np.zeros(len(rewards))\n",
    "    G = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        G = rewards[t] + gamma * G\n",
    "        returns[t] = G\n",
    "    \n",
    "    alpha = 0.000000005 / (1 + 0.00005 * episode)\n",
    "    \n",
    "    for t in range(len(obss)):\n",
    "        grad = compute_log_gradient(obss[t], actions[t])  \n",
    "        K += alpha * grad * returns[t]  \n",
    "    print(f\"Episode {episode}, Total Reward: {ret}, Best Reward: {bret}, K:{K}\" )\n",
    "    \n",
    "print(bK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the environment with parallel envs\n",
    "envname=\"InvertedPendulum-v5\" \n",
    "env = gym.make_vec(envname,num_envs=8,render_mode='rgb_array',vectorization_mode='sync')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=0.99\n",
    "num_episodes=500\n",
    "action_std=0.3\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(4, 10,device='cuda')  # input size 1 → output size 10\n",
    "        self.layer2 = nn.Linear(10, 10,device='cuda')  \n",
    "        self.layer3 = nn.Linear(10, 1,device='cuda')  # 10 → 1\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = torch.relu(self.layer1(state))\n",
    "        output = torch.relu(self.layer2(output))\n",
    "        output = self.layer3(output)\n",
    "        return output\n",
    "\n",
    "def rollout(env, policy_net, max_steps=1000):\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    alive = torch.ones(8, dtype=torch.bool, device='cuda') ## (8,)\n",
    "    states, actions, rewards, log_probs = [], [], [], []\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "\n",
    "        if not alive.any(): ## alive.any(): True if any entry in alive is True\n",
    "            break\n",
    "\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32,device='cuda') ## (8,4) on GPU \n",
    "        mean = policy_net(state_tensor) ## (8,4)→(8,1)\n",
    "        dist = Normal(mean, action_std)\n",
    "        action = dist.sample() ## (8,1)\n",
    "        log_prob = dist.log_prob(action).sum(dim=-1) ## (8,1).sum(dim=-1) -> (8,)\n",
    "        \n",
    "        next_state, reward, done, truncated, _ = env.step(action.cpu().numpy())\n",
    "        done = np.logical_or(done, truncated)\n",
    "        \n",
    "        # mask out dead envs\n",
    "        reward = torch.tensor(reward, device='cuda') ##(8,)\n",
    "        reward = reward * alive.float()\n",
    "        mask=alive.float().unsqueeze(1) ## (8,)-> (8,1)\n",
    "        action = action * mask ##(8,1)\n",
    "    \n",
    "        states.append(state) ## (N,4)\n",
    "        actions.append(action) ## (N,1)\n",
    "        rewards.append(reward) ## (N,)\n",
    "        log_probs.append(log_prob * alive.float()) ## (N,)\n",
    "\n",
    "        alive = alive & (~torch.tensor(done, device='cuda'))\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    return states, actions, rewards, log_probs\n",
    "\n",
    "def compute_returns(rewards, gamma=0.99):\n",
    "    T = len(rewards)\n",
    "    returns = torch.zeros(T, dtype=torch.float32, device='cuda')\n",
    "    G = 0.0\n",
    "    for t in range(T-1, -1, -1):    \n",
    "        G = rewards[t] + gamma * G\n",
    "        returns[t] = G\n",
    "    return returns\n",
    "\n",
    "policy_net=Network()\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=1e-2)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    states, actions, rewards, log_probs=rollout(env,policy_net)\n",
    "    all_returns=[]\n",
    "    # Split rewards per environment\n",
    "    for env_idx in range(8):\n",
    "        # Collect rewards only for env_idx\n",
    "        rewards_per_env = [rewards[t][env_idx] for t in range(len(rewards))]\n",
    "        returns = compute_returns(rewards_per_env, gamma)\n",
    "        all_returns.append(returns)\n",
    "\n",
    "    returns_tensor = torch.stack(all_returns, dim=1)##(N,8) \n",
    "    returns_tensor=(returns_tensor-returns_tensor.mean())/(returns_tensor.std()+0.0000001)\n",
    "    log_probs_tensor = torch.stack(log_probs) ##(N,8)\n",
    "    loss = -(log_probs_tensor * returns_tensor).sum() ##scalar\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    rewards_tensor = torch.stack(rewards, dim=0)  ## (N, 8)\n",
    "    total_per_env      = rewards_tensor.sum(dim=0) ## (8,)\n",
    "    avg_total_reward   = total_per_env.mean().item() ## scalar\n",
    "\n",
    "    print(\"Avg total return:\", avg_total_reward,\"episode:\", episode)\n",
    "    if avg_total_reward >= 999:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "terminated = [False]\n",
    "ret = 0\n",
    "\n",
    "while not all(terminated):\n",
    "    state = obs\n",
    "\n",
    "    with torch.no_grad():\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32,device='cuda') # shape: [1, 4]\n",
    "        mean= policy_net(state_tensor)\n",
    "        action = mean.cpu().numpy()  # Use mean as deterministic action at test time\n",
    "    # Step in the environment\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    ret += reward[0].item()\n",
    "    \n",
    "    # Visualize\n",
    "    render_notebook(env, 0, f\"{ret=:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
